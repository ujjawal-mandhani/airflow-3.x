# version: '3.9'
services:
  airflow-cont:
    build: ./airflow/
    container_name: airflow-cont
    environment:
      DISPLAY: ${DISPLAY}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth'
      AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
      AIRFLOW__CORE__TEST_CONNECTION: Enabled
      AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ujjawal
      _AIRFLOW_WWW_USER_PASSWORD: ujjawalpassword
      AIRFLOW_UID: 50000
      JAVA_HOME: "/usr"
      TZ: Asia/Kolkata
      AWS_CONFIG_FILE: /home/.aws/config
      AWS_SHARED_CREDENTIALS_FILE: /home/.aws/credentials
      AWS_PROFILE: airflow
      AWS_DEFAULT_REGION: ap-south-1
      # AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      # AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-cont:8080/execution/'
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT: 8793
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8080
      AIRFLOW__WEBSERVER__WEB_SERVER_HOST: airflow-cont
      AIRFLOW__LOGGING__WORKER_LOG_SERVER_HOST: celery-worker-1
      AIRFLOW__LOGGING__REMOTE_LOGGING: 'false'
      AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg' # Uncommenting This is throwing an error for jwt_token needs to be set
      # AIRFLOW__CORE__FERNET_KEY: ZnAVSIyFSgCKnr4lrpA5crFKY_yktX5bCa3_F7ZnEdI=
      # AIRFLOW__ELASTICSEARCH__HOST: elasticsearch-cont:9200
      # AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: http://elasticsearch-cont:9200/airflow
      # AIRFLOW__ELASTICSEARCH__INDEX_PATTERNS: airflow*
      # AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
      # AIRFLOW__ELASTICSEARCH__LOG_ID_TEMPLATE: "{dag_id}-{task_id}-{execution_date}-{try_number}"
      # AIRFLOW__ELASTICSEARCH__WRITE_STDOUT: 'true'
      # AIRFLOW__ELASTICSEARCH__ENDPOINT: http://elasticsearch-cont:9200
      # AIRFLOW__ELASTICSEARCH__INDEX_NAME: airflow
      # AIRFLOW__ELASTICSEARCH__JSON_FORMAT: 'true'
      # AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: elasticsearch_default
      HADOOP_CONF_DIR: '/opt/hadoop/etc/hadoop'
    volumes:
      - ./mnt/dags:/opt/airflow/dags
      - ./mnt/config:/opt/airflow/config/
      - /home/ujjawalmandhani/.aws:/home/.aws:ro
      - ./mnt/elasticsearch_creation.py:/home/elastic/elasticsearch_creation.py
      - ./airflow/hadoop_config:/opt/hadoop/etc/hadoop
      - /home/ujjawalmandhani/.mozilla/firefox:/home/.mozilla
      - /home/ujjawalmandhani/.config/google-chrome/:/home/.google-chrome
      - /tmp/.X11-unix:/tmp/.X11-unix
      - /home/ujjawalmandhani/.Xauthority:/root/.Xauthority
      - ./mnt/airflow-dbt-spark:/opt/airflow/airflow-dbt-spark
      - ./mnt/.dbt/:/home/airflow/.dbt/
      - ./mnt/ssh/id_rsa:/home/airflow/.ssh/id_rsa
      - ./mnt/ssh/id_rsa.pub:/home/airflow/.ssh/id_rsa.pub
      # - /home/ujjawalmandhani/Desktop/MyPassport/home/ujjawal/Desktop/postgred_db/torrent:/home/torrents/html
    ports:
      - 9057:8080
    networks:
      - my_shared_network
    restart: always
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "--fail",
          "http://airflow-cont:8080/api/v2/monitor/health"
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  redis:
    image: redis:latest
    container_name: redis
    restart: always
    ports:
      - 6379:6379
    networks:
      - my_shared_network
  
  celery-worker-1:
    build: ./airflow/
    hostname: celery-worker-1
    depends_on:
      - airflow-cont
      - redis
      - postgres
    command: bash -c "airflow db migrate && airflow celery worker"
    # command: bash -c "airflow db migrate && celery --app airflow.providers.celery.executors.celery_executor worker --hostname celery-worker-1@%h"
    # command: tail -f /dev/null
    entrypoint: ''
    volumes: 
      - /home/ujjawalmandhani/.aws:/home/.aws:ro
      - ./mnt/dags:/opt/airflow/dags
      - ./mnt/config:/opt/airflow/config/
      - ./airflow/hadoop_config:/opt/hadoop/etc/hadoop
      - /home/ujjawalmandhani/.mozilla/firefox:/home/.mozilla
      - /home/ujjawalmandhani/.config/google-chrome/:/home/.google-chrome
      - /tmp/.X11-unix:/tmp/.X11-unix
      - /home/ujjawalmandhani/.Xauthority:/root/.Xauthority
      # - /home/ujjawalmandhani/Desktop/MyPassport/home/ujjawal/Desktop/postgred_db/torrent:
      - ./staticfiles:/opt/airflow/torrent/html/
      - /run/user/1000/pulse/native:/run/user/1000/pulse/native  
      - ./mnt/airflow-dbt-spark:/opt/airflow/airflow-dbt-spark
      - ./mnt/.dbt/:/home/airflow/.dbt/
      - ./mnt/ssh/id_rsa:/home/airflow/.ssh/id_rsa
      - ./mnt/ssh/id_rsa.pub:/home/airflow/.ssh/id_rsa.pub
    restart: always
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - 8793:8793
    environment:
      DISPLAY: ${DISPLAY}
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AWS_CONFIG_FILE: /home/.aws/config
      AWS_SHARED_CREDENTIALS_FILE: /home/.aws/credentials
      AWS_PROFILE: airflow
      AWS_DEFAULT_REGION: ap-south-1
      AIRFLOW__CORE__SKIP_DB_INIT: 'true'
      AIRFLOW__WEBSERVER__BASE_URL: http://airflow-cont:8080
      AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT: 8793
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs
      AIRFLOW__WEBSERVER__WEB_SERVER_HOST: airflow-cont
      AIRFLOW__LOGGING__WORKER_LOG_SERVER_HOST: celery-worker-1
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-cont:8080/execution/'
      AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
      HADOOP_CONF_DIR: '/opt/hadoop/etc/hadoop'
      # AIRFLOW__CORE__FERNET_KEY: ZnAVSIyFSgCKnr4lrpA5crFKY_yktX5bCa3_F7ZnEdI=
      # AIRFLOW__ELASTICSEARCH__HOST: elasticsearch-cont:9200
      # AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: elasticsearch_default
      # AIRFLOW__ELASTICSEARCH__INDEX_PATTERNS: airflow*
      # AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
      # AIRFLOW__ELASTICSEARCH__LOG_ID_TEMPLATE: "{dag_id}-{task_id}-{execution_date}-{try_number}"
      # AIRFLOW__ELASTICSEARCH__WRITE_STDOUT: 'true'
      # AIRFLOW__ELASTICSEARCH__ENDPOINT: http://elasticsearch-cont:9200
      # AIRFLOW__ELASTICSEARCH__INDEX_NAME: airflow
      # AIRFLOW__ELASTICSEARCH__JSON_FORMAT: 'true'
    networks:
      - my_shared_network
  postgres:
    image: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./postgres/postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    ports:
      - 19057:5432
    networks:
      - my_shared_network

  # elasticsearch-cont:
  #   # image: elasticsearch:8.15.3
  #   image: docker.elastic.co/elasticsearch/elasticsearch:7.17.13
  #   container_name: elasticsearch-cont
  #   environment:
  #     - discovery.type=single-node
  #     - ELASTIC_PASSWORD=elasticsearchpassword # user name is "elastic"
  #     - xpack.security.enabled=false
  #   ports:
  #     - 9200:9200
  #     - 9300:9300
  #   networks:
  #     - my_shared_network
  #   mem_limit: 2g
  #   volumes:
  #     - ./elasticsearch:/usr/share/elasticsearch/data
  #   restart: always
  #   healthcheck:
  #     test:
  #       [
  #         "CMD-SHELL",
  #         "curl --silent --fail localhost:9200/_cluster/health || exit 1"
  #       ]
  #     interval: 40s
  #     timeout: 10s
  #     retries: 5

  # kibana-cont:
  #   # image: kibana:8.12.2
  #   image: docker.elastic.co/kibana/kibana:7.17.13
  #   ports:
  #     - 5601:5601
  #   environment:
  #     - SERVERNAME=kibana
  #     - ELASTICSEARCH_HOSTS=http://elasticsearch-cont:9200
  #   mem_limit: 2g
  #   volumes:
  #     - ./kibana:/usr/share/kibana/data
  #   restart: always
  #   depends_on:
  #     - elasticsearch-cont
  #   networks:
  #     - my_shared_network

networks:
  my_shared_network:
    # driver: bridge
    external: true
